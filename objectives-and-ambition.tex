\subsection{Objectives and ambition}

\label{sect:objectives}

As the values of Open and Reproducible Science are adopted by
governments, funding agencies, research institutes, and society,
there are practical challenges to implementing such practices on a global scale.

Two of the key societal goals of Open Science are facilitation of

\begin{compactenum}
\item \textbf{verification} of results, improving the reliability and trustworthiness of scientific output, and
\item \textbf{reuse} of research products, enabling commercial exploitation and/or derivative research
\end{compactenum}

If results are merely \emph{available}, however,
much of the value of Open Science remains unrealised.
\myemph{Practical reproducibility} is required for efficient, widespread fulfillment of Open Science goals.
But what does it mean to be reproducible?
Without the \emph{practical} ability to verify results, results are not
any more likely to be verified.
Without the \emph{practical} ability to reuse work, results are not
any more likely to be exploited in new research or commercial work.

In \TheProject, we focus on `practical reproducibility',
specifically focusing on computational results, such as computer simulation, data
processing, data analysis, and creation of figures and tables in publications.
We differentiate `technical reproducibility,'
where enough information is \emph{technically present} to reproduce the work given sufficient effort,
from `practical reproducibility,'
where effort to reproduce the work is not a burden \cite{binder}.


Computational reproducibility is a major challenge facing all scientific domains,
from social sciences to life sciences, physical sciences, engineering, and digital humanities.
Almost every area of academia must spend some time computing,
ranging from large-scale computer simulations to basic data analysis to produce a figure in a publication.

All of these face a common reproducibility challenge: \textbf{the computational environment},
or the collection of software used to produce the output.
To reproduce the work, a \textbf{sufficiently similar} computational environment must be produced to re-execute the code.
This may be a specific version of an application, or a large collection of software dependencies,
as is common in data science fields.

In order to meet the goals or policies of Open and Reproducible science,
\myemph{researchers need tools}. Ideally, those tools should

\begin{compactenum}
\item be freely available and open source
\item meet researchers where they are, as much as possible
\end{compactenum}

Binder is a project providing open source tools to solve this computational environment reproducibility challenge.
It has already proven useful for at least tens of thousands of researchers, educators, developers, and students,
and we believe it has the potential to facilitate practical reproducibility for millions more,
including into institutions, funding agencies, and policy makers.
We aim to realise that potential.

% Increasingly, Open Science and Reproducibility are declared as values or requirements
% at research institutions, governments, and funding agencies.
% In order to meet these goals or requirements,
% researchers need \myemph{tools} that help them accomplish the tasks required for reproducible work.

It is in this context that we set our objectives to \textbf{find, experiment, and mainstream Binder tools as concrete solutions and best-practices to increase the reproducibility of research}:

\TOWRITE{Feedback}{
Objectives should be SMART: Specific, Measurable, Achievable, Relevant, Time-bound
}

\begin{table}[H]
  \label{tab:objectives-tasks}
  \caption{
  Our objectives and their relationship to the work programme}
  \begin{tabular}{>{\raggedright}m{.2\textwidth}|m{.4\textwidth}|m{.4\textwidth}}

    \hline

    \myemph{Objective} & \myemph{Description} & \myemph{Relation to work programme}

    \\\hline

    \label{obj:reproducibility} \myemph{Evaluate and facilitate better computational
    reproducibility and FAIR data}
    &
    Tools for reproducibility must be evaluated by how successfully they produce the correct environment.
    We will provide evaluation tools for reproducibility,
    and use these tools to guide improvements to the Binder tools,
    solving known issues where we can improve the \myemph{reproducibility of computational environments}
    used for science, and facilitate \myemph{FAIR data practices}.
    &
    The core of this effort is to \textbf{develop, validate, pilot and deploy practices and practical tools for funders, publishers and scientists}.
    The robustness of these tools is key to their utility and value,
    and thereby adoption in research communities.
    \\\hline

    \label{obj:broaden}
    \myemph{Enable reproducibility using common tools in a wider variety of environments}
    &
    We develop generic tools for reproducible software environments,
    but have identified several areas where the Binder tools do not yet meet user needs,
    such as in traditional HPC environments, or users with large data.
    We shall address those gaps so that tools and strategies can be shared by a wider variety of communities,
    aligning effort and reducing necessary duplication.
    &
    In order to \textbf{promote uptake, greater collaboration, and increased alignment of the activities of stakeholders},
    it is most efficient when knowledge and tools can be shared.
    When tools do not work for significant fractions of the research community
    they must develop their own, often similar tools,
    as has often been the case for the HPC community.

    \\\hline

    \label{obj:demonstrators}
    \myemph{Demonstrate reproducibility in specific scientific applications}
    &
    We will demonstrate the utility of the Binder tools for achieving reproducible research
    in a variety of scientific domains,
    which serves both to inform and motivate improvements to the system,
    and as illustration and reference for others to follow.
    &
    By collaborating across domains, we \textbf{promote uptake, greater collaboration, and increased alignment of the activities of stakeholders}.
    Further, by publishing working examples,
    we contribute to an \textbf{open knowledge base of results, methodologies and interventions on the drivers and consequences of reproducibility} in these specific domains,
    to be used as reference or followed by others in the same domain
    and other domains with similar challenges.

    \\\hline

    \label{obj:education}
    \myemph{Educate researchers about reproducible practices}
    &
    Develop best practice guidelines for reproducible science, and disseminate this by
    educating the research communities about reproducible practices and available
    tools for reproducible publications and policies. Reach out to scientists, and
    the wider research communities and reproducibility stakeholders to encourage
    engagement with this project.
    &
    By collecting expertise and guidelines, we contribute an \textbf{open knowledge base of results, methodologies and interventions}
    for computational reproducibility.
    \\\hline

  \end{tabular}
\end{table}


\subsubsection{Ambition}

\TheProject's main goal is to \myemph{improve the global reproducibility of
  scientific results} with a focus on those aspects of the research process that
are supported by computation and software, such as computer simulation, data
processing, data analysis and creation of figures and tables in publications.

We plan to achieve this goal through
\begin{compactitem}
\item educating researchers about good reproducible practices, and
\item make it easier to perform computational research in a reproducible way
  through improving and developing relevant software tools.
\end{compactitem}

\subsubsection{State of the Art}

To make computational research reproducible, we generally need to make available
(i)~required data, (ii)~the required software, (iii)~the protocol that explains
how to process the data to obtain the result that is to be reproduced. Using
services such as Zenodo, it is possible to deposit such archives with a DOI and
make reference to them in publications.

In order to reproduce the results, it should be possible for anybody to take such an archive of a research
output, and to carry out the two necessary steps:
\begin{compactitem}
\item Step 1 to install the required software environment, and
\item Step 2 to follow the protocol to reproduce the results from the archived data.
\end{compactitem}

It is of particular value if Steps 1 \emph{and} 2 can be \emph{carried out
  automatically} by executing some kind of script or program that is part of the
archive. First, if the automatic execution is possible, we know that there is a
complete description of the protocol included in the archive, and that no
mistake is made in trying to follow the protocol. Second, the automatic
execution saves time.

There are two major approaches emerging to achieve this reproducibility: (a) to use a
workflow tool or environment that caters to a given use case (for example
~\cite{reana2019} \TODO{cite other workflow systems}). Or (b) to use standard
(software engineering) computing tools and conventions (git, make, python, perl,
bash, \ldots) to specify the compute environment and reproduce steps piecemeal.

The workflow tool approach (a) is more robust, but requires `all-in' adoption by authors and reproducers alike,
and may not suit all use cases. It can also have associated `vendor lock-in' effects
- once a tool is adopted for one piece, it must be used for all associated work.
A consequence of this more tailored approach is reduced re-use and duplication of effort.
As a workflow tool may not meet the needs of a community,
that community must then build its own tool, or be left without appropriate tools
if they lack the resources or expertise to build their own.
Workflow tools also often \emph{dictate} a significant amount of how researchers perform their work,
which can inhibit adoption when it does not suit their existing patterns.

The standard computing tools approach (b) is generic, but not accessible to all researchers
as it requires substantial training or experience to be effective,
and prone to errors or incompleteness when executed,
leading to the "Works for Me" problem,
where they author may have no trouble reproducing their work,
but they have not effectively communicated the requirements such that others can do the same.
The gap between in research expertise and practical need when it comes to these tools has led to the creation of whole industries of remedial skills training.
The loose coupling of tools and modular choices make it more flexible (covering more use cases),
but more difficult to follow robustly.
Identifying which tools to use and following appropriate installation is a challenge
for both authors, who must communicate requirements clearly without knowing the reader's context,
and readers, who must find, understand, and follow potentially complex directions, even assuming they were complete and correct.

\medskip Researchers who use the Jupyter Notebook to orchestrate their
computational research (see Figure~\ref{fig:jeodpp}) can achieve this automatic
reproducibility with little additional effort~\cite{Beg2021}: they use the Notebook document as
the protocol of their analysis (Step 2), which can be executed automatically.
They can make use of the Binder tool (Section~\ref{seq:project-binder}) and the
associated mybinder.org service that has
been designed by the Jupyter team to automatically create the appropriate
software environment (step 1) in which the notebook can be executed.

\begin{figure}[tb]
  \centering\includegraphics[height=0.2\textheight]{images/jeodpp.png}
  \centering\includegraphics[height=0.2\textheight]{images/jeodpp-demo.jpg}
  \caption{\emph{Left}: The Joint Research Centre (JRC) Earth Observation
    Data and Processing Platform (JEODPP) is a user of the
    Jupyter Notebook (source:
    \url{https://cidportal.jrc.ec.europa.eu/home/}), where it features
    at the top of the pyramid to help users orchestrate layers of data
    analysis software and hardware. \emph{Right}: An example
    service in which an interactive visualisation is provided through
    the Jupyter Notebook rendering of the density map of the ships
    detected from Sentinel-1 images over the Mediterranean sea during
    the period October 2014 to September 2016. \cite[Figure
    6]{Soille2018}. \label{fig:jeodpp}}
  \TODO{Do we want to keep the figure? The left side is useful, and it is good
    to have some images.}
\end{figure}

\subsubsection{Beyond state of the art}

In this project, we will focus on the \emph{reproduction of the
  software environment} (Step 1) which is a prerequisite for any attempt to
reproduce the actual research outputs. In particular, we want to make the
creation of this computational environment \emph{automatic}.

We will go beyond the current state of the art by
\begin{compactitem}
\item enhancing the existing Binder software, already widely used by authors of Jupyter notebooks,
  directly improving the reproducibility of research created by the substantial
  community of researchers using such notebooks,
\item extending the capabilities of the Binder tools so that its capability to
  create arbitrary software environments automatically can be realised more broadly beyond the
  Jupyter user community,
\item adding new capabilities to the Binder tools that enable new reproducibility
  use cases, such as those that need access to large data sets, access to restricted sets,
  and reproducibility for High Performance Computing,
\item enabling the Binder tools to run on the desktop computer of individual
  researchers (rather than having to rely on central or institutional
  installations such as mybinder.org).
\end{compactitem}

\subsubsection{Motivation - Why?}\label{sec:motivation-why}

We focus on the computational reproducibility because it is a real
obstacle for practical reproducibility.

First, it affects the majority of all researchers: there are estimates that over
92\% of all researchers work with \emph{research software} and over 50\% develop
their own~\cite{Hettrick2014}. Where experiments drive the research, this is
often data processing, analysis, and plotting. Each of those computational
research cases needs a software environment in which the actual processing can
be carried out. The software environment may consist of somewhat standard
packages (for example use of a Python, R, or Julia plotting library), or it may
include tailored programs that have been developed especially for the study.

Second, software packaging and management is a technically challenging topic,
and we cannot expect 92\% of all researchers to master it -- so we believe there
is a clear need to support this with appropriate tooling.

The complexity arises in parts from the increasing age of archived studies, and
also in the often unusual combinations of research software and libraries that
need to be combined for a particular study. Other difficulties include that a
reproduction typically needs to be done on a different computer, perhaps even on
a different operating system. If, say, a plotting library is used, then it may
change its interface or behaviour over time, so it is important to install
exactly the right version of the plotting library, before a reproduction of
results is attempted using it.

Third, being able to re-create the appropriate software environment is a
pre-requisite before any actual reproduction of results can be attempted: it
would be inefficient to educate researchers what data and programs to archive,
if in the future nobody (or only very few highly trained people) will be able to
execute those scripts.

Finally, we think that there are low hanging fruits: the work proposed here will
make it possible create computational environments automatically for
\emph{existing data archives}: the Binder philosophy is to understand existing standards for software
specification, and to build a software environment based on those standards.
Where researchers have used the existing software specification already, Binder tools
will work immediately on their archived files. This means that (i)~a researcher
putting together a well organised archive does not need to know about Binder tools,
yet the researcher who wants to reproduce the results later can use Binder tools to
automate the recreation of the software environment. This also means that
(ii)~improvements we propose in this work, will make some existing archives (that
have been created in the past) more easily reproducible.

%(It is part of our training programme to educate about the importance and methods
%for software specification.)



\begin{draft}

\TODO{HF: Move this content to 3.2}

\subsubsection{Excellence}

We have a diverse and interdisciplinary team driving this project, in which we
bring together research software developers, researchers, research support staff,
and educators -- each world class in their domain -- with the common vision to
work towards better open source tools for better reproducibility in science.

Through our multiple and interdisciplinary roles, we see the same process of
generating reproducible research outputs from the perspective of different
stakeholders, and can propose and develop solutions that are \emph{useful and
  practical} in real-world research environments.

We extend our own experience through our wide network of collaborators and
colleagues and will -- as part of the execution of this project -- seek
constant exchange with and feedback from different additional stakeholders in
our \emph{Community Engagement Panel} to shape the work of the
\TheProject project. As a team experienced in developing open source software,
we expect to be able to go beyond this and attract development contributions
from volunteers to support this project.

The existing Binder tools -- which are the baseline for this project --
originate from Project Jupyter. We have core Jupyter and Binder developers in
our team, and thus direct access to developer expertise and experience.

\end{draft}
%  research effectiveness for many of their millions of users.

%   Our aim is to re-use the work that has gone into the development of the Binder
%   tools, and to make this functionality for automatic creation of computational
%   environments available to researchers outside the Jupyter user community to
%   improve reproducibility at a wider scale.




\subsubsection{Technical Readiness Level (TRL)}

TRL is unusual for Binder tools, because they are tools that can be used in many ways,
and they can be described as having a different TRL \emph{in different contexts}.

The Binder software and service demonstration at mybinder.org is TRL 6,
where scope \emph{excludes} long-term robustness.
The Binder software for deploying services with authenticated access to data is TRL 3.
We will bring it to TRL 5 or 6.
The Binder tool repo2docker when targeting robust reproducible environments is TRL 4.
We will bring it to TRL 6.
