\subsection{Objectives and ambition}

\label{sect:objectives}

As the values of Open and Reproducible Science are adopted by
governments, funding agencies, research institutes, and society,
there are practical challenges to implementing such practices on a global scale.

Two of the key societal goals of Open Science are facilitation of

\begin{compactenum}
\item \textbf{verification} of results, improving the reliability and trustworthiness of scientific output, and
\item \textbf{reuse} of research products, enabling commercial exploitation and/or derivative research
\end{compactenum}

If results are merely \emph{available}, however,
much of the value of Open Science remains unrealised.
\myemph{Practical reproducibility} is required for efficient, widespread fulfillment of Open Science goals.
But what does it mean to be reproducible?
Without the \emph{practical} ability to verify results, results are not
any more likely to be verified.
Without the \emph{practical} ability to reuse work, results are not
any more likely to be exploited in new research or commercial work.

In \TheProject, we focus on `practical reproducibility',
specifically focusing on computational results, such as computer simulation, data
processing, data analysis, and creation of figures and tables in publications.
We differentiate `technical reproducibility,'
where enough information is \emph{technically present} to reproduce the work given sufficient effort,
from `practical reproducibility,'
where effort to reproduce the work is not a burden \cite{binder}.


Computational reproducibility is a major challenge facing all scientific domains,
from social sciences to life sciences, physical sciences, engineering, and digital humanities.
Almost every area of academia must spend some time computing,
ranging from large-scale computer simulations to basic data analysis to produce a figure in a publication.

All of these face a common reproducibility challenge: \textbf{the computational environment},
or the collection of software used to produce the output.
To reproduce the work, a \textbf{sufficiently similar} computational environment must be produced to re-execute the code.
This may be a specific version of an application, or a large collection of software dependencies,
as is common in data science fields.

In order to meet the goals or policies of Open and Reproducible science,
\myemph{researchers need tools} to address the challenge. Ideally, those tools should
\begin{compactenum}
\item be freely available and open source
\item meet researchers where they are, as much as possible
\end{compactenum}

Binder is a project providing open source tools to solve this computational environment reproducibility challenge.
It has already proven useful for at least tens of thousands of researchers, educators, developers, and students,
and we believe it has the potential to facilitate practical reproducibility for millions more,
including in institutions, funding agencies, and policy makers.
We aim to realise that potential.

% Increasingly, Open Science and Reproducibility are declared as values or requirements
% at research institutions, governments, and funding agencies.
% In order to meet these goals or requirements,
% researchers need \myemph{tools} that help them accomplish the tasks required for reproducible work.

It is in this context that we set our objectives to \textbf{find, experiment, and mainstream Binder tools as concrete solutions and best-practices to increase the reproducibility of research}:

\TOWRITE{Feedback}{
Objectives should be SMART: Specific, Measurable, Achievable, Relevant, Time-bound
}

\begin{table}[H]
  \label{tab:objectives-tasks}
  \caption{
  Our objectives and their relationship to the work programme}
  \begin{tabular}{>{\raggedright}m{.2\textwidth}|m{.4\textwidth}|m{.4\textwidth}}

    \hline

    \myemph{Objective} & \myemph{Description} & \myemph{Relation to work programme}

    \\\hline

    \label{obj:reproducibility} \myemph{Evaluate and facilitate better computational
    reproducibility and FAIR data}
    &
    Tools for reproducibility must be evaluated by how successfully they produce the correct environment.
    We will provide evaluation tools for reproducibility,
    and use these tools to guide improvements to the Binder tools,
    solving known issues where we can improve the \myemph{reproducibility of computational environments}
    used for science, and facilitate \myemph{FAIR data practices}.
    &
    The core of this effort is to \textbf{develop, validate, pilot and deploy practices and practical tools for funders, publishers and scientists}.
    The robustness of these tools is key to their utility and value,
    and thereby adoption in research communities.
    \\\hline

    \label{obj:broaden}
    \myemph{Enable reproducibility using common tools in a wider variety of environments}
    &
    We develop generic tools for reproducible software environments,
    but have identified several areas where the Binder tools do not yet meet user needs,
    such as in traditional HPC environments, or users with large data.
    We shall address those gaps so that tools and strategies can be shared by a wider variety of communities,
    aligning effort and reducing necessary duplication.
    &
    In order to \textbf{promote uptake, greater collaboration, and increased alignment of the activities of stakeholders},
    it is most efficient when knowledge and tools can be shared.
    When tools do not work for significant fractions of the research community
    they must develop their own, often similar tools,
    as has often been the case for the HPC community.

    \\\hline

    \label{obj:demonstrators}
    \myemph{Demonstrate reproducibility in specific scientific applications}
    &
    We will demonstrate the utility of the Binder tools for achieving reproducible research
    in a variety of scientific domains,
    which serves both to inform and motivate improvements to the system,
    and as illustration and reference for others to follow.
    &
    By collaborating across domains, we \textbf{promote uptake, greater collaboration, and increased alignment of the activities of stakeholders}.
    Further, by publishing working examples,
    we contribute to an \textbf{open knowledge base of results, methodologies and interventions on the drivers and consequences of reproducibility} in these specific domains,
    to be used as reference or followed by others in the same domain
    and other domains with similar challenges.

    \\\hline

    \label{obj:education}
    \myemph{Educate researchers about reproducible practices}
    &
    Develop best practice guidelines for reproducible science, and disseminate this by
    educating the research communities about reproducible practices and available
    tools for reproducible publications and policies. Reach out to scientists, and
    the wider research communities and reproducibility stakeholders to encourage
    engagement with this project.
    &
    By collecting expertise and guidelines, we contribute an \textbf{open knowledge base of results, methodologies and interventions}
    for computational reproducibility.
    \\\hline

  \end{tabular}
\end{table}


\subsubsection{Ambition}

\TheProject's main goal is to \myemph{improve the global reproducibility of
  scientific results} with a focus on those aspects of the research process that
are supported by computation and software, such as computer simulation, data
processing, data analysis and creation of figures and tables in publications.

We plan to achieve this goal through
\begin{compactitem}
\item educating researchers about good reproducible practices, and
\item making it easier to perform computational research in a reproducible way
  through improving and developing relevant software tools.
\end{compactitem}

\subsubsection{State of the Art}

To make computational research reproducible, we generally need to make available
(i)~required data, (ii)~the required software, (iii)~the protocol that explains
how to process the data to obtain the result that is to be reproduced. Using
services such as Zenodo, it is possible to deposit such archives with a DOI and
make reference to them in publications.

In order to reproduce the results, it should be possible for anybody to take such an archive of a research
output, and to carry out the two necessary steps:
\begin{compactitem}
\item Step 1 to install the required software, and
\item Step 2 to follow the protocol to reproduce the results from the archived data.
\end{compactitem}

It is of particular value if Steps 1 \emph{and} 2 can be \emph{carried out
  automatically} by executing some kind of script or program that is part of the
archive. First, if the automatic execution is possible, we know that there is a
complete description of the protocol included in the archive, and that no
mistake is made in trying to follow the protocol. Second, the automatic
execution saves time.

There are two major approaches emerging to achieve this reproducibility: (a) to use a
workflow tool or environment that caters to a given use case (for example
~\cite{reana2019} \TODO{cite other workflow systems})
and encapsulates the full process for strong reproducility guarantees.
Or (b) to use standard (software engineering) computing tools and conventions
(git, make, python, perl, bash, \ldots)
to specify the compute environment and reproduce steps piecemeal.

The workflow tool approach (a) is more robust, but requires `all-in' adoption by authors and reproducers alike,
and cannot suit all use cases. It can also have associated `vendor lock-in' effects
- once a tool is adopted for one piece, it must be used for all associated work.
A consequence of this more tailored approach is reduced re-use and duplication of effort.
As a workflow tool may not meet the needs of a community,
that community must then build its own tool, or be left without appropriate tools
if they lack the resources or expertise to build their own.
Workflow tools also often \emph{dictate} a significant amount of how researchers perform their work,
which can inhibit adoption when it does not suit their existing patterns.

The standard computing tools approach (b) is generic, but not accessible to all researchers
as it requires substantial training or experience to be effective,
and prone to errors or incompleteness when executed,
leading to the "Works for Me" problem,
where the author may have no trouble reproducing their own work,
but they have not effectively communicated the requirements such that others can do the same.
The gap
% gap between what and what?
between in research expertise and practical need when it comes to these tools has led to the creation of whole industries of remedial skills training.
The loose coupling of tools and modular choices make it more flexible (covering more use cases),
but more difficult to follow robustly.
Identifying which tools to use and following appropriate installation is a challenge
for both authors, who must communicate requirements clearly without knowing the reader's context,
and readers, who must find, understand, and follow potentially complex directions, even assuming they were complete and correct.

\medskip Researchers who use the Jupyter Notebook to orchestrate their
computational research (see Figure~\ref{fig:jeodpp}) can achieve this automatic
reproducibility with little additional effort~\cite{Beg2021}: they use the Notebook document as
the protocol of their analysis (Step 2), which can be executed automatically.
They can make use of the Binder tools (Section~\ref{seq:project-binder}) and/or the
associated mybinder.org service that has
been designed by the Jupyter team to automatically create the appropriate
software environment (step 1) in which the notebook can be executed.

\begin{figure}[tb]
  \centering\includegraphics[height=0.2\textheight]{images/jeodpp.png}
  \centering\includegraphics[height=0.2\textheight]{images/jeodpp-demo.jpg}
  \caption{\emph{Left}: The Joint Research Centre (JRC) Earth Observation
    Data and Processing Platform (JEODPP) is a user of the
    Jupyter Notebook (source:
    \url{https://cidportal.jrc.ec.europa.eu/home/}), where it features
    at the top of the pyramid to help users orchestrate layers of data
    analysis software and hardware. \emph{Right}: An example
    service in which an interactive visualisation is provided through
    the Jupyter Notebook rendering of the density map of the ships
    detected from Sentinel-1 images over the Mediterranean sea during
    the period October 2014 to September 2016. \cite[Figure
    6]{Soille2018}. \label{fig:jeodpp}}
  \TODO{Do we want to keep the figure? The left side is useful, and it is good
    to have some images.}
\end{figure}

\repotodocker{} is a tool that aims to \emph{automate existing practices} for reproducible environment specification.
It finds \emph{standard} environment specifications and produces \emph{typical} installation commands,
constructing an environment in a Docker container.
\repotodocker{} is successful enough to be widely adopted,
but many shortcomings have been identified,
especially when a repository contains an \emph{incomplete} specification.
A common issue is to produce an environment that works when published,
but which may produce a non-working environment at a later point in time,
due to version drift and insufficiently strict specifications.
Additionally, community practices not yet supported by \repotodocker{} have been proposed by their respective community members,
while the \repotodocker{} team has not had the funding support to incorporate and maintain.

The BinderHub software exposes \repotodocker{} and Jupyter as a service,
allowing one-click reproduction of published environments for \emph{interactive exploration}.
BinderHub is shown to work well,
but has limited application due to technical limitations,
such as its reliance on the Kubernetes deployment platform,
or challenges with combining the convenience of BinderHub's anonymous-by-default model
with authenticated and/or performant access to large data or compute resources.

BinderHub instances such as mybinder.org are extremely convenient,
but being hosted services they do not offer the digital sovereignty
of private execution on one's own computational resources,
be they a local machine or cloud or on-premises clusters.

While the \repotodocker{} terminal utility \emph{can} technically be deployed anywhere,
the technical expertise required is dramatically greater than a single click on a website.


\subsubsection{Beyond state of the art}

In this project, we will focus on the \emph{reproduction of the
software environment} (Step 1) which is a prerequisite for any attempt to
reproduce the actual research outputs. In particular, we want to make the
creation of this computational environment \emph{automatic}, \emph{generic} and \emph{robust},
especially for long-term preservation.

We will go beyond the current state of the art by bringing the following selected improvements:
\begin{compactitem}
\item currently producing valid computational environments from published scientific publications and/or existing online repositories is difficult and usually requires manual handling of the process which is cumbersome and time consuming. Improving the robustness of \repotodocker{}'s ability to produce computational environments, long after the publication and/or release,
      through testing and development, as well as taking additional context information into account, such as repository publication date will enable the production of computational environment to be seamless;
\item the existing Binder software is already widely used in the Jupyter user community but focused on the reproducibility of Jupyter notebooks that may not fit everyone’s needs: for instance mastering one of the many programming languages (> 40) supported by Jupyter is required and limits take-up by some communities. Extending the Binder software beyond the notebooks will undoubtedly attract new community of users and can facilitate transfer of knowledge between academia and industry;
\item The current Binder tools rely on Kubernetes and deploying a Binder service requires technical skills that are beyond many institutional or companies IT support staff. As a result, most researchers rely on existing deployments that are overloaded and cannot cope with the huge demand, and they often quickly disregard Binder as a viable solution for creating reproducible computational environments. Being able to use \repotodocker{} anywhere e.g. from the user’s personal laptop (Binder@Home) to the most powerful High Performance Computers (Binder@HPC) by removing technical restrictions such as the dependence on Kubernetes, and implementing more community practices for reproducible environments will open new ways of using Binder software that are more in line with the current needs of end-users;
\item another important bottleneck is the need to access and to reuse very large and complex data sets (sometimes with restricted access permissions) that are published and deposited on (domain) specific long-term archives. Examples (to read and process such datasets) can be provided by end-users (either as part of the dataset itself or separately) but the usage of the Binder software and/or existing public Binder deployments for this use case is not currently supported (such amount of data cannot be easily and efficiently moved to the Binder resources). Ad-hoc or domain specific solutions (for instance the usage of cloud optimized data formats and associated catalogs such as intake or STAC by the Pangeo Geoscience community) have been explored by diverse communities but are technically difficult and not generic enough to be adopted by everyone. The Binder software will be extended to facilitate the plugin of external long-term archive resources and enable the publication and reuse of large and complex datasets (software close to the data and Binder service).
\end{compactitem}


\subsubsection{Motivation - Why?}\label{sec:motivation-why}

We focus on the computational reproducibility because it is a real
obstacle for practical reproducibility.

First, it affects the majority of all researchers: there are estimates that over
92\% of all researchers work with \emph{research software} and over 50\% develop
their own~\cite{Hettrick2014}. Where experiments drive the research, this is
often data processing, analysis, and plotting. Each of those computational
research cases needs a software environment in which the actual processing can
be carried out. The software environment may consist of somewhat standard
packages (for example use of a Python, R, or Julia plotting library), or it may
include tailored programs that have been developed specifically for a study.

Second, software packaging and management is a technically challenging topic,
and we cannot expect 92\% of all researchers to master it -- so we believe there
is a clear need to support them with appropriate tooling.

The complexity arises in parts from the increasing age of archived studies, and
also in the often unusual combinations of research software and libraries that
need to be combined for a particular study. Other difficulties include that a
reproduction typically needs to be done on a different computer, perhaps even on
a different operating system. If, say, a plotting library is used, then it may
change its interface or behaviour over time, so it is important to install
exactly the right version of the plotting library, before a reproduction of
results is attempted using it.

Third, being able to re-create the appropriate software environment is a
pre-requisite before any actual reproduction of results can be attempted: it
would be inefficient to educate researchers what data and programs to archive,
if in the future nobody (or only very few highly trained people) will be able to
execute those scripts.

Finally, we think that there are low-hanging fruits: the work proposed here will
make it possible to create computational environments automatically for
\emph{existing data archives}: the Binder philosophy is to support existing standards for software
specification, and to build a software environment based on those standards.
Where researchers have used the existing software specification already, Binder tools
will work immediately on their archived files. This means that (i)~a researcher
putting together a well organised archive does not need to know about Binder tools,
yet the researcher who wants to reproduce the results later can use Binder tools to
automate the recreation of the software environment. This also means that
(ii)~improvements we propose in this work, will make some existing archives (that
have been created in the past) more easily reproducible.

%(It is part of our training programme to educate about the importance and methods
%for software specification.)






\subsubsection{Technical Readiness Level (TRL)}

TRL is unusual for Binder tools, because they are tools that can be used in many ways,
and they can be described as having a different TRL \emph{in different contexts}.

The Binder software and service demonstration at mybinder.org is TRL 6,
where scope \emph{excludes} long-term robustness.
The Binder software for deploying services with authenticated access to data is TRL 3.
We will bring it to TRL 5 or 6.
The Binder tool repo2docker when targeting robust reproducible environments is TRL 4.
We will bring it to TRL 6.
